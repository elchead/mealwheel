{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mappings:\n",
    "    def __init__(self, dataset: Dataset) -> None:\n",
    "        \"\"\"\n",
    "        userid: user_id\n",
    "        row: internal user id\n",
    "        itemid: recipe_id\n",
    "        column: internal recipe id\n",
    "        \"\"\"\n",
    "        userid2row, _, itemid2col, _ = dataset.mapping()\n",
    "        self.userid2row = userid2row\n",
    "        self.itemid2col = itemid2col\n",
    "        # Invert dictionaries to get mapping in other direction\n",
    "        self.row2userid = {value: key for key, value in self.userid2row.items()}\n",
    "        self.col2itemid = {v: k for k, v in self.itemid2col.items()}\n",
    "\n",
    "class LightFMResizable(LightFM):\n",
    "    \"\"\"A LightFM that resizes the model to accomodate new users,\n",
    "    items, and features\"\"\"\n",
    "\n",
    "    def fit_partial(\n",
    "        self,\n",
    "        interactions,\n",
    "        user_features=None,\n",
    "        item_features=None,\n",
    "        sample_weight=None,\n",
    "        epochs=1,\n",
    "        num_threads=1,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        try:\n",
    "            self._check_initialized()\n",
    "            self._resize(interactions, user_features, item_features)\n",
    "        except ValueError:\n",
    "            # This is the first call so just fit without resizing\n",
    "            pass\n",
    "\n",
    "        super().fit_partial(\n",
    "            interactions,\n",
    "            user_features,\n",
    "            item_features,\n",
    "            sample_weight,\n",
    "            epochs,\n",
    "            num_threads,\n",
    "            verbose,\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _resize(self, interactions, user_features=None, item_features=None):\n",
    "        \"\"\"Resizes the model to accommodate new users/items/features\"\"\"\n",
    "\n",
    "        no_components = self.no_components\n",
    "        no_user_features, no_item_features = interactions.shape  # default\n",
    "\n",
    "        if hasattr(user_features, \"shape\"):\n",
    "            no_user_features = user_features.shape[-1]\n",
    "        if hasattr(item_features, \"shape\"):\n",
    "            no_item_features = item_features.shape[-1]\n",
    "\n",
    "        if (\n",
    "            no_user_features == self.user_embeddings.shape[0]\n",
    "            and no_item_features == self.item_embeddings.shape[0]\n",
    "        ):\n",
    "            return self\n",
    "\n",
    "        new_model = clone(self)\n",
    "        new_model._initialize(no_components, no_item_features, no_user_features)\n",
    "\n",
    "        # update all attributes from self._check_initialized\n",
    "        for attr in (\n",
    "            \"item_embeddings\",\n",
    "            \"item_embedding_gradients\",\n",
    "            \"item_embedding_momentum\",\n",
    "            \"item_biases\",\n",
    "            \"item_bias_gradients\",\n",
    "            \"item_bias_momentum\",\n",
    "            \"user_embeddings\",\n",
    "            \"user_embedding_gradients\",\n",
    "            \"user_embedding_momentum\",\n",
    "            \"user_biases\",\n",
    "            \"user_bias_gradients\",\n",
    "            \"user_bias_momentum\",\n",
    "        ):\n",
    "            # extend attribute matrices with new rows/cols from\n",
    "            # freshly initialized model with right shape\n",
    "            old_array = getattr(self, attr)\n",
    "            old_slice = [slice(None, i) for i in old_array.shape]\n",
    "            new_array = getattr(new_model, attr)\n",
    "            new_array[tuple(old_slice)] = old_array\n",
    "            setattr(self, attr, new_array)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted(scores: np.ndarray, top_n):\n",
    "        \"\"\"\n",
    "        Get the top indices sorted descendingly from the scores list array.\n",
    "        Args:\n",
    "        scores: An array with scores.\n",
    "        top_n: The number of top scores to be returned.\n",
    "        Returns:\n",
    "            ScoringList: The first element of the tuple is the index where the score was\n",
    "                    in the original array, the second element is the score itself.\n",
    "        \"\"\"\n",
    "        best_idxs = np.argpartition(scores, -top_n)[-top_n:]\n",
    "        return sorted(zip(best_idxs, scores[best_idxs]), key=lambda x: -x[1])\n",
    "def load_data(path=\"\"):\n",
    "    \"\"\"\n",
    "    Loads the following files:\n",
    "        raw_recipes\n",
    "        model    Params:\n",
    "        path: Path to folder with the files. If path=\"\", files must be in the same folder as this notebook.\n",
    "    \"\"\"\n",
    "    raw_recipes = pd.read_csv(path + \"RAW_recipes.csv\", sep=\",\")\n",
    "    filename = \"dataset.pkl\"\n",
    "    with open(path + filename, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "    filename = \"recommendation_model.pkl\"\n",
    "    with open(path + filename, 'rb') as file:\n",
    "        model = pickle.load(file)   \n",
    "    mappings = Mappings(dataset)\n",
    "    return raw_recipes,mappings, model, dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function get_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(a, new_user_recipe_ids, path):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    a: number of recommendations you want\n",
    "    new_user_recipe_ids: list 5 liked recipe_id\n",
    "    path: Path to folder with the files. If path=\"\", files must be in the same folder as this notebook\n",
    "    Output:\n",
    "    output: a recommendations as a json format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    raw_recipes, mappings, model, dataset = load_data(path)\n",
    "    \n",
    "    # fit_partial new user\n",
    "    new_user = pd.DataFrame({\n",
    "        \"user_id\":  [1] * len(new_user_recipe_id), \n",
    "        \"recipe_id\":  new_user_recipe_id})\n",
    "    dataset.fit_partial(users=new_user[\"user_id\"], items=new_user[\"recipe_id\"])\n",
    "    new_interactions, _ = dataset.build_interactions(new_user.to_records(index=False))\n",
    "    model.fit_partial(interactions=new_interactions)\n",
    "\n",
    "    user_id = 1\n",
    "    # Get the internal id (or: row) for this user, the number of items in the dataset & the scores for each item (for our user)\n",
    "    user_row = mappings.userid2row[user_id]\n",
    "    _, n_items = dataset.interactions_shape()\n",
    "    item_columns = np.arange(n_items)\n",
    "    scores = model.predict(user_ids=user_row, item_ids=item_columns)\n",
    "            \n",
    "    sorted_scores_top = get_top_sorted(scores, a)\n",
    "    \n",
    "    # Add results to a DataFrame\n",
    "    recommendations = pd.DataFrame(sorted_scores_top, columns=[\"internal_item_id\", \"score\"])\n",
    "    recommendations[\"user_id\"] = user_id\n",
    "    recommendations[\"recipe_id\"] = recommendations[\"internal_item_id\"].apply(lambda x: mappings.col2itemid[x])\n",
    "    recommendations = recommendations[[\"user_id\", \"recipe_id\", \"score\"]]\n",
    "    \n",
    "    to_adrian = recommendations.set_index('recipe_id').join(raw_recipes.set_index('id'))\n",
    "    to_adrian.drop(['name'], axis = 1, inplace = True)\n",
    "    to_adrian.drop(['contributor_id'], axis = 1, inplace = True)\n",
    "    to_adrian.drop(['submitted'], axis = 1, inplace = True)\n",
    "    to_adrian.drop(['user_id'], axis = 1, inplace = True)\n",
    "\n",
    "    # Convert to json \n",
    "    output = to_adrian.to_json(orient=\"records\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"score\":2.2480459213,\"minutes\":10,\"tags\":\"[\\'15-minutes-or-less\\', \\'time-to-make\\', \\'course\\', \\'preparation\\', \\'for-1-or-2\\', \\'low-protein\\', \\'beverages\\', \\'easy\\', \\'dietary\\', \\'low-sodium\\', \\'low-cholesterol\\', \\'low-calorie\\', \\'low-carb\\', \\'low-in-something\\', \\'number-of-servings\\', \\'3-steps-or-less\\']\",\"nutrition\":\"[111.8, 10.0, 41.0, 2.0, 3.0, 14.0, 3.0]\",\"n_steps\":3,\"steps\":\"[\\'stir together the hot espresso , creamer , and cinnamon in a glass measuring cup\\', \\'pour into 2 mugs\\', \\'add a scoop of ice cream to each and sprinkle with cardamom\\']\",\"description\":\"trying to stay away from the coffee shops and make my own drinks and save money!!\",\"ingredients\":\"[\\'brewed espresso\\', \\'non-dairy coffee creamer\\', \\'ground cinnamon\\', \\'vanilla ice cream\\', \\'ground cardamom\\']\",\"n_ingredients\":5}]'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_recipe_id = [4065, 10123, 295797, 108524, 10045]\n",
    "get_recommendations(1, new_user_recipe_id, \"C:/Users/leaed/Documents/Techlabs/Mealwheeldata/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
